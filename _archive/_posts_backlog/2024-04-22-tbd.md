---
layout: post
title:  "Relevance of transformer models in healthcare"
author: sandy
categories: [ healthcare, EHR, AI ]
image: assets/images/2024-03/iStock-1025882206_license.jpg
featured: false
hidden: false
---

Some recent studies[] have made it into the media highlighting limitations of LLMs, perhaps casting a shadow on the true potential of generative AI.  As I've written before, healthcare is arguably at an inflection point where innovation is needed, but in the right places.  As a business leader or technologist, it is important to ask:  In what ways are these AI models superior to their predecessors?  What problem can this AI model actually solve?  In this post, I take a deeper look into the intuition behind transformer models, the healthcare applications where it is being used, and their feasability in healthcare.

## A primer on transformer models
There is no shortage of resources to learn about transformers such as the Generative Pre-trained Transformer (GPT), and I will not go into the details here.  However, I found this [tutorial](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0) fairly intuitive in understanding transformers, and it serves as a good backdrop for their application in healthcare.  I have added additional resources in the reference section.

At a high-level, transfomer models make use of the [attention mechanism](https://www.kdnuggets.com/2021/01/attention-mechanism-deep-learning-explained.html), which I like to think of as a computer's ability to focus on specific parts of an input sequence to accomplish a specific ML task (e.g., predict an output word).  A specific type of mechanism, called [self-attention](https://arxiv.org/abs/1706.03762), is a technique that [mathematically weighs](https://armanasq.github.io/nlp/self-attention) the importance between different words in an input sequence.  

https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html
https://machinelearningmastery.com/the-transformer-attention-mechanism/
https://dzone.com/articles/a-deep-dive-into-the-transformer-architecture-the
https://www.datacamp.com/tutorial/how-transformers-work

Here is a simple visual from the above-mentioned tutorial of how attention mechanism works and how it compares to LTSM and RNN architectures:

![AttentionMechanism](/assets/images/2024-04/atm.png)

![LSTM](/assets/images/2024-04/lstm.png)

![RNN](/assets/images/2024-04/rnn.png)

As shown above, this technique overcomes the limitation of previous NLP models which ran into the problem of having short-term memory.  The result is that a computer can more accurately retain the meaning of longer sentences. 


## Transformers in healthcare
Now that we see how the transformer model has succeeeded over previous NLP architectures, let's take a look at how it is being applied in healthcare.  

In the paper ['Transformer Models in Healthcare: A Survey and Thematic Analysis of Potentials, Shortcomings and Risks'](https://link.springer.com/article/10.1007/s10916-024-02043-5), the authors highlight 


## Are transformers that much more powerful in healthcare?

## Conclusion

## References
+ <https://www.datacamp.com/tutorial/how-transformers-work>
+ https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0
+ https://dzone.com/articles/a-deep-dive-into-the-transformer-architecture-the
+ <https://www.kdnuggets.com/2021/01/attention-mechanism-deep-learning-explained.html>
+ <https://machinelearningmastery.com/the-transformer-attention-mechanism)>