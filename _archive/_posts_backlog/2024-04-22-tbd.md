---
layout: post
title:  "Relevance of transformer models in healthcare"
author: sandy
categories: [ healthcare, EHR, AI ]
image: assets/images/2024-03/iStock-1025882206_license.jpg
featured: false
hidden: false
---

Some recent studies[] have made it into the media highlighting limitations of LLMs, perhaps casting a shadow on the true potential of generative AI.  As I've written before, healthcare is arguably at an inflection point where innovation is needed, but in the right places.  As a business leader or technologist, I think its important to ask:  In what ways are these AI models superior to their predecessors?  What are the most applicable use cases?  In this post, I take a deeper look into healthcare applications using transfomer models, and evaluate their usefulness in healthcare.

## A primer on transformer models
There is no shortage of resources to learn about transformer models such as GPT, and I will not go into the details here.  However, I found this [tutorial](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0) particularly intuitive in understanding transformers, and it serves as a good backdrop for their application in healthcare. 

At a high-level, transfomer models make use of the [attention mechanism](https://www.kdnuggets.com/2021/01/attention-mechanism-deep-learning-explained.html), which one can think of as a computer's ability to focus on specific parts of an input sequence to accomplish a specific ML task (e.g., predict an output word).  A specific type of mechanism, called [self-attention](https://arxiv.org/abs/1706.03762), enables a computer to [mathematically weigh](https://armanasq.github.io/nlp/self-attention/#:~:text=Self-attention%20enables%20models%20to%20weigh%20the%20importance%20of,when%20making%20predictions%20or%20capturing%20dependencies%20between%20words.) the importance (i.e., pay more attention to) between different words in a sequence.

Here is a nice visual of how attention mechanism works and how it bests RNN and LTSM architectures:

[image]

As shown above, this technique overcomes the limitation of previous NLP models such as LTSM and RNN, which struggled to process long input sentences.  The result of this technique is that a computer can retain meaning of longer sentences.

*If you really want to nerd out on the mathematics behind the transformer attention mechanisms, I highly recommend this [tutorial](https://machinelearningmastery.com/the-transformer-attention-mechanism). 


## Experiments using transformers in healthcare
Now that we have understood the how the transformer model has succeeeded over previous NLP architectures, let's take a look at how it is being applied in healthcare.  

In considering LLMs, 


## Are transformers that much more powerful in healthcare?

## Conclusion

## References
+ <https://www.datacamp.com/tutorial/how-transformers-work>
+ https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0
+ https://dzone.com/articles/a-deep-dive-into-the-transformer-architecture-the
+ <https://www.kdnuggets.com/2021/01/attention-mechanism-deep-learning-explained.html>