---
layout: post
title:  "Relevance of transformer models in healthcare"
author: sandy
categories: [ healthcare, EHR, AI ]
image: assets/images/2024-03/iStock-1025882206_license.jpg
featured: false
hidden: false
---

Some recent studies[] have made it into the media highlighting limitations of LLMs, perhaps casting a shadow on the true potential of generative AI.  As I've written before, healthcare is arguably at an inflection point where innovation is needed, but in the right places.  As a business leader or technologist, it is important to ask:  In what ways are these AI models superior to their predecessors?  What problem can this AI model actually solve?  In this post, I take a deeper look into transformer models and their potential utility in healthcare.

## A primer on transformer models
There is no shortage of resources to learn about transformers such as the Generative Pre-trained Transformer (GPT), and I will not go into the details here.  However, I found this [tutorial](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0) fairly intuitive in understanding transformers, and it serves as a good backdrop for their application in healthcare.  I have added additional resources in the reference section.

At a high-level, transfomer models make use of the [self-attention mechanism](https://arxiv.org/abs/1706.03762), which is a technique that [calculates](https://armanasq.github.io/nlp/self-attention) the importance between different pairs of words in an input sequence, and processes them in parallel.  This enables the model to more accurately focus on the most relevant words when generating new output.  Pleae refer to these technical blogs for more details: 

https://www.kdnuggets.com/2021/01/attention-mechanism-deep-learning-explained.html
https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html
https://machinelearningmastery.com/the-transformer-attention-mechanism/
https://dzone.com/articles/a-deep-dive-into-the-transformer-architecture-the
https://www.datacamp.com/tutorial/how-transformers-work
https://humanoid.tools/articles/understanding-self-attention-and-positional-encoding-in-language-models/#:~:text=The%20mechanism%20works%20by%20calculating%20attention%20scores%20between,relevant%20words%20when%20making%20predictions%20or%20generating%20output.


Here is a simple visual from the above-mentioned tutorial of how attention mechanism works and how it compares to LTSM and RNN architectures:

![AttentionMechanism](/assets/images/2024-04/atm.png)

![LSTM](/assets/images/2024-04/lstm.png)

![RNN](/assets/images/2024-04/rnn.png)

As shown above, this technique overcomes the limitation of previous NLP models which ran into the problem of having short-term memory.  The result is that a computer can more accurately retain the meaning of longer sentences. 


## Are transformers that much more powerful in healthcare?
Now that we see how the transformer model has succeeeded over previous NLP architectures, let's take a look at how it is being evaluated in healthcare.  

In the February 2024 paper ['Transformer Models in Healthcare: A Survey and Thematic Analysis of Potentials, Shortcomings and Risks'](https://link.springer.com/article/10.1007/s10916-024-02043-5), authors from Europe highlighted 

https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-024-02459-6#author-information

https://www.nature.com/articles/s41746-024-01010-1#author-information

## Reasons to remain optimistic

## Conclusion

## References
+ <https://www.datacamp.com/tutorial/how-transformers-work>
+ https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0
+ https://dzone.com/articles/a-deep-dive-into-the-transformer-architecture-the
+ <https://www.kdnuggets.com/2021/01/attention-mechanism-deep-learning-explained.html>
+ <https://machinelearningmastery.com/the-transformer-attention-mechanism)>

+ <https://devennn.github.io/2020/self-and-multihead-attention>