---
layout: post
title:  "Relevance of transformer models in healthcare"
author: sandy
categories: [ healthcare, EHR, AI ]
image: assets/images/2024-03/iStock-1025882206_license.jpg
featured: false
hidden: false
---

Some recent studies[] have made it into the media highlighting limitations of LLMs, perhaps casting a shadow on the true potential of generative AI.  As I've written before, healthcare is arguably at an inflection point where innovation is needed, but in the right places.  As a business leader or technologist, it is important to ask:  In what ways are these AI models superior to their predecessors?  What problem can this AI model actually solve?  In this post, I take a deeper look into transformer models and their potential utility in healthcare.

## A primer on transformer models
There is no shortage of resources to learn about transformers such as the Generative Pre-trained Transformer (GPT).  I will not go into the details here, but provide links to blogs that best illustrate key concepts.  

At a high-level, transfomer models make use of the [self-attention mechanism](https://arxiv.org/abs/1706.03762), which is a ML technique that [calculates](https://armanasq.github.io/nlp/self-attention) the importance between different pairs of words in an input sequence, and processes them in parallel.  This enables the model to more accurately focus on the most relevant words when generating new output.  Check out these technical blogs for further details:
- [Attention to improve machine translation models](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention)
- [Transformers (illustrated)](https://jalammar.github.io/illustrated-transformer)
- [Self-attention & multi-head attention](https://devennn.github.io/2020/self-and-multihead-attention) 

Here is a simplified [visual](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0) of how attention mechanism works, and how it compares to LTSM and RNN architectures:

![AttentionMechanism](/assets/images/2024-04/atm.png)

![LSTM](/assets/images/2024-04/lstm.png)

![RNN](/assets/images/2024-04/rnn.png)

As shown above, this technique overcomes the limitation of previous NLP models which ran into the problem of having short-term memory.  The result is that a computer can more accurately retain the meaning of longer input sequences when generating new output. 


## Are transformers that much more powerful in healthcare?
Now that we see how the transformer model has succeeeded over previous NLP architectures, let's take a look at how it is being evaluated in healthcare.  I should caveat this section by saying that this field is evolving very rapidly, so what has been published in recent months will likely be challenged and revised.  My goal here is to provide a small snapshot of studies that I found compelling.  

In the February 2024 paper ['Transformer Models in Healthcare: A Survey and Thematic Analysis of Potentials, Shortcomings and Risks'](https://link.springer.com/article/10.1007/s10916-024-02043-5), authors from Europe highlighted some of the key benefits and challenges with genAI technology:

![Fig1](/assets/images/2024-04/Fig1.png)

https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-024-02459-6#author-information

https://www.nature.com/articles/s41746-024-01010-1#author-information

## Reasons to remain optimistic

## Conclusion

## References
+ <https://www.datacamp.com/tutorial/how-transformers-work>
+ https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0
+ https://dzone.com/articles/a-deep-dive-into-the-transformer-architecture-the
+ <https://www.kdnuggets.com/2021/01/attention-mechanism-deep-learning-explained.html>
+ <https://machinelearningmastery.com/the-transformer-attention-mechanism)>

https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html
https://humanoid.tools/articles/understanding-self-attention-and-positional-encoding-