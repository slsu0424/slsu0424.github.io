---
layout: post
title:  "Relevance of transformer models in healthcare"
author: sandy
categories: [ healthcare, EHR, AI ]
image: assets/images/2024-03/iStock-1025882206_license.jpg
featured: false
hidden: false
---

Some recent studies[] have made it into the media highlighting limitations of LLMs, perhaps casting a shadow on the true potential of generative AI.  As I've written before, healthcare is arguably at an inflection point where innovation is needed, but in the right places.  As a business leader or technologist, I think its important to ask:  In what ways are these AI models superior to their predecessors?  What are the most applicable use cases?  In this post, I take a deeper look into healthcare applications using transfomer models, and evaluate their usefulness in healthcare.

## A primer on transformer models
There is no shortage of resources to learn about transformer models such as BERT or GPT, and I will not go into the details here.  However, I found this [tutorial](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0) particularly intuitive in understanding transformers, and it serves as a good backdrop for their application in healthcare. 

At a high-level, transfomer models make use of the [attention mechanism](https://www.kdnuggets.com/2021/01/attention-mechanism-deep-learning-explained.html), which one can think of as a computer's ability to focus on specific parts of an input sequence to accomplish a specific ML task (e.g., predict an output word).  A specific type of mechanism, called [self-attention](https://arxiv.org/abs/1706.03762), enables a computer to [mathematically weigh](https://armanasq.github.io/nlp/self-attention/#:~:text=Self-attention%20enables%20models%20to%20weigh%20the%20importance%20of,when%20making%20predictions%20or%20capturing%20dependencies%20between%20words.) the importance (i.e., pay more attention to) between different words in a sequence.

Here is a nice visual from the above-mentioned tutorial of how attention mechanism works and how it compares to LTSM and RNN architectures:

![AttentionMechanism](/assets/images/2024-04/atm.png)

![LSTM](/assets/images/2024-04/lstm.png)

![RNN](/assets/images/2024-04/rnn.png)

As shown above, this technique overcomes the limitation of previous NLP models which ran into the problem of having short-term memory.  The result of this technique is that a computer can retain meaning of longer sentences. 


## Transformers in healthcare
Now that we have understood how the transformer model has succeeeded over previous NLP architectures, let's take a look at how it is being applied in healthcare.  

In the paper ['Transformer Models in Healthcare: A Survey and Thematic Analysis of Potentials, Shortcomings and Risks'](https://link.springer.com/article/10.1007/s10916-024-02043-5), the authors highlight 


## Are transformers that much more powerful in healthcare?

## Conclusion

## References
+ <https://www.datacamp.com/tutorial/how-transformers-work>
+ https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0
+ https://dzone.com/articles/a-deep-dive-into-the-transformer-architecture-the
+ <https://www.kdnuggets.com/2021/01/attention-mechanism-deep-learning-explained.html>
+ <https://machinelearningmastery.com/the-transformer-attention-mechanism)>