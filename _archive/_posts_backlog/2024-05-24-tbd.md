---
layout: post
title:  "The med-mal debate in the era of generative AI"
author: sandy
categories: [ healthcare, EHR, AI ]
image: assets/images/2024-03/iStock-1025882206_license.jpg
featured: false
hidden: false
---

I had not explored too much into AI ethics & regulation as of late, but I was intrigued by this [article](https://www.politico.com/news/2024/03/24/who-pays-when-your-doctors-ai-goes-rogue-00148447) that shed light into the [on-going discussion](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8452365) of accountability when AI is used for patient care.  The reason this is important now is because AI tools are proliferating in healthcare more than ever before.  Should physicians ultimately be on the hook?  What role do regulators play?  What would it take to integrate AI tools to become standards of care?  In this post, I review the current discussions around these questions and recommended paths going forward.  

## Who makes the final call when AI is involved?
Physicians should be on the hook to understand the technology they are using when it comes to clinical decision-making.  At least, that's the point of view from the technology companies.  Physicians have been put in this unfavorable position in the past, subject to lawsuits when physicians have followed erroneous software recommendations.  What's different this time around is that physicians could also be held accountable if they *did not* make use of AI at all.  Let's say a LLM were at a physician's disposal, would it be considered patient negligence if they did not make use of its recommendations?  What recourse does a physician have in this case?

According to Dr. Michelle Mello, a Stanford health law scholar:

>Unregulated tools could be more vulnerable to lawsuits, according to Mello, because they aren’t protected by the “preemption doctrine,” which bars some claims against regulated devices on the theory that they’re known to be safe enough and effective enough to have received FDA clearance.

I found this comment striking because to my knowledge, there are no known LLMs that have been [approved](https://www.fdli.org/2023/12/tech-support-fdas-evolving-regulatory-plan-for-drug-and-device-enabling-software/#_edn15) by the FDA.  To note, this should not be mistaken for AI-enabled devices that use deep learning algorithms that have been recently approved by the FDA (i.e., [Eko Low Ejection Fraction Tool (ELEFT)](https://www.accessdata.fda.gov/cdrh_docs/pdf23/K233409.pdf)) 

If there is increased liability for using such tools, then there is little incentive for adopting these in real-world settings.

## Ensuring governance
If one wants to get a good look at what governance for AI in healthcare should look like, I found Dr. Mello's [February 2024 testimony](https://www.finance.senate.gov/imo/media/doc/02082024_mello_testimony.pdf) to the U.S. Senate Committee on Finance to be an excellent resource.  In the document, she highlights key learnings from evaluating AI tools:

1.  Not all healthcare organizations have the resources to develop AI review processes.  Government can help here.
2. Governance should involve how an AI algorithm works within a clinical workflow.  Who is to ensure that clinical staff uses AI the way that it was intended by developers?
3. The government should establish standards for using AI tools in healthcare.  However, regulation needs to be agile and should reside within a consensus-building entity (i.e., Coalition for Health AI).  Federally regulated programs, such as Medicare, Medicaid, the VA health system, should adhere to these standards.

Lastly, Dr. Mello notes that health insurers also need AI regulation, and points to the example of unregulated AI used by Medicare Advantage plans to [deny care](https://www.statnews.com/2023/03/13/medicare-advantage-plans-denial-artificial-intelligence) for seniors in need.


## Conclusion
Throughout this recent journey in researching the societal effects of AI in healthcare, its become clear that we are navigating choppy waters with the rapid development of AI.  As a technologist, we should not only care about bringing potentially life-changing solutions to market, but also consider its ethical implications and plans for responsible use.  I remain optimistic that with consensus-built AI standards, there will be more (rightful) scrutiny from every angle.  Its not out of reach to say that AI can improve health outcomes for all, but implementing it will require investment across the ecosystem. 

## References
+ <https://www.politico.com/news/2024/03/24/who-pays-when-your-doctors-ai-goes-rogue-00148447>
+ <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8452365>
+ <https://www.fdli.org/2023/12/tech-support-fdas-evolving-regulatory-plan-for-drug-and-device-enabling-software/#_edn15>
+ <https://www.accessdata.fda.gov/cdrh_docs/pdf23/K233409.pdf>
+ <https://www.finance.senate.gov/imo/media/doc/02082024_mello_testimony.pdf>
+ <https://www.statnews.com/2023/03/13/medicare-advantage-plans-denial-artificial-intelligence>

https://www.politico.com/newsletters/future-pulse/2024/05/17/a-leader-for-the-ai-skeptics-00158592
https://www.politico.com/newsletters/politico-pulse/2024/02/20/the-fdas-ai-quandary-00142114
https://www.statnews.com/2024/02/07/ai-assurance-laboratories-onc-fda-equity/
https://journalofethics.ama-assn.org/issue/artificial-intelligence-health-care
https://www.healthaffairs.org/content/forefront/executive-order-artificial-intelligence-impact-health-care
https://jamanetwork.com/journals/jama/article-abstract/2812613
