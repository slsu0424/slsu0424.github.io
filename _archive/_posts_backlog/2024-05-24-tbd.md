---
layout: post
title:  "The med-mal debate in the era of generative AI"
author: sandy
categories: [ healthcare, EHR, AI ]
image: assets/images/2024-03/iStock-1025882206_license.jpg
featured: false
hidden: false
---

I had not delved too much into AI ethics as of late, but I was intrigued by this [article](https://www.politico.com/news/2024/03/24/who-pays-when-your-doctors-ai-goes-rogue-00148447) that sheds light into the [on-going discussion](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8452365/) of accountability when AI is used for patient care.  The reason this is important now is because AI tools are proliferating in healthcare more than ever before.  Should physicians ultimately be on the hook?  What role do regulators play?  What would it take to integrate AI tools to become standards of care?  Will tech companies register AI tools as SaMD?  In this post, I explore the current discussions around these questions and recommended paths going forward.  

## Who makes the final call when AI is involved?
Physicians should be on the hook to understand the technology they are using when it comes to clinical decision-making.  At least, that's the point of view from the technology companies.  Physicians have been put in this unfavorable position in the past, subject to lawsuits when physicians have followed erroneous software recommendations.  What's different this time around is that physicians could also be held accountable if they *did not* make use of AI at all.  Let's say a LLM were at a physician's disposal, would it be considered patient negligence if they did not make use of its recommendations?  What recourse does a physician have in this case?

According to Dr. Michelle Mello, a Stanford health law scholar:

>Unregulated tools could be more vulnerable to lawsuits, according to Mello, because they aren’t protected by the “preemption doctrine,” which bars some claims against regulated devices on the theory that they’re known to be safe enough and effective enough to have received FDA clearance.

If there is increased liability for using such tools, then there is little incentive for adopting these in real-world settings.

## Ensuring governance
If one wants to get a good look at what governance for AI in healthcare could look like, I found Dr. Mello's [February 2024 testimony](https://www.finance.senate.gov/imo/media/doc/02082024_mello_testimony.pdf) to the U.S. Senate Committee on Finance to be an excellent resource.  In it, she highlights 










## Conclusion



## References
https://www.politico.com/newsletters/future-pulse/2024/05/17/a-leader-for-the-ai-skeptics-00158592
https://www.politico.com/newsletters/politico-pulse/2024/02/20/the-fdas-ai-quandary-00142114
https://www.statnews.com/2024/02/07/ai-assurance-laboratories-onc-fda-equity/
https://journalofethics.ama-assn.org/issue/artificial-intelligence-health-care
https://www.healthaffairs.org/content/forefront/executive-order-artificial-intelligence-impact-health-care
https://jamanetwork.com/journals/jama/article-abstract/2812613
