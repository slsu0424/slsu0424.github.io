---
layout: post
title:  "Black box algorithms: Are they a problem in healthcare?"
author: sandy
categories: [ AI, healthcare ]
image: assets/images/2023-08/healthcare-is-keeping-the-fax-machine-alive.png
---
After writing my last post, I was intrigued to learn a bit more about the black box phenomenon in AI models.  More pressing is understanding their implication for healthcare (perceived vs. actual).  This is not to say that AI has not already been embedded into medical products today, judging by the increasing number of FDA-approved AI algorithms.  However, I think it is important to understand the distinction of the algorithms embedded into such products.  In this post, I aim to provide a technical lens into this topic.  

## A database of FDA-approved AI algorithms
For those that are not aware, the FDA regularly tracks and publishes [AI/ML-enabled medical devices](https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-and-machine-learning-aiml-enabled-medical-devices) that have gained regulatory approval.  Fortunately, I was also able to dig up a 2020 article in [Nature](https://www.nature.com/articles/s41746-020-00324-0) which organizes these device algorithms into a database.  Although it looks like it has not been updated since mid-2021, it is a good reference point for analysis.  

In reviewing this [database](https://medicalfuturist.com/fda-approved-ai-based-algorithms), I decided to focus on algorithms focused on cardiology.  Eko Devices Inc. developed the Eko Analysis Software (EAS), which was approved by the FDA on January 15, 2020.  According to their [summary](https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfpmn/pmn.cfm?ID=K192004) document, the tool is described as follows:

>The Eko Analysis Software is a cloud-based software API that allows a user to upload
synchronized ECG and heart sound/phonocardiogram (PCG) data for analysis. The software
uses several methods to interpret the acquired signals including signal processing and artificial
neural networks. The API can be electronically interfaced, and perform analysis with data
transferred from multiple mobile or computer based applications. 

I will focus on the neural networks component.  

## Discriminative vs. Generative models
Before I dive into discriminative and generative models, I found it helpful to review the main approaches to perform machine learning tasks: supervised and unsupervised learning.  

At a high level, supervised learning uses labeled inputs (x) and outputs (y) to fit a model.  Here, the machine algorithm estimates, or learns, a function that best relates the input ("labeled data") to output ("labels").  

![supervised](/assets/images/2024-06/supervised.png){:.centered}

{:.image-caption}
*Credit: Denecke, K., May, R. & Rivera-Romero, O. Transformer Models in Healthcare: A Survey and Thematic Analysis of Potentials, Shortcomings and Risks. J Med Syst 48, 23 (2024).  Used under [CC BY 4.0.](https://creativecommons.org/licenses/by/4.0)*

In contrast, unsupervised learning has no labeled inputs nor outputs.  Here, the machine learning algorithm estimates a function that finds similarity among the input samples, and groups them based on that similarity as the output.  

![unsupervised](/assets/images/2024-06/unsupervised.png){:.centered}

The models that fall under each approach can be generally categorized as [discriminative or generative](https://learnopencv.com/generative-and-discriminative-models/).  By discriminative, the model aims to separate data points into different classes and learn the decision boundaries.  By generative, the model aims to generate new data points similar to the data it was trained on:

![gen_disc_model-1](/assets/images/2024-06/gen_disc_model-1.png){:.centered}

[Mathematically](https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-supervised-learning), we typically see discriminative models used in supervised learning, where the goal is to estimate the conditional probability of the output (y) given the input (x), thus P(y|x).  Generative models are typically used in unsupervised learning, where the goal is to estimate the joint probability of both the input (x) and output (y), thus P(xâˆ©y).  New data points can be generated by sampling from the underlying probability distribution:

![math](/assets/images/2024-06/math.png){:.centered}

While neural networks are typically associated with unsupervised learning, the lines are somewhat blurred as discussed in some [online forums](https://stats.stackexchange.com/questions/403968/linking-generative-discriminative-models-to-supervised-and-unsupervised-learnin).  Thus, I would say they can be either discriminative or generative models, based on the specific use case.  For example, neural networks can be used for image classification (convolutional neural networks) or for generating new data (transformers).

As for the Eko Analysis System, the underlying neural networks are probably discriminative models.  According to their [FAQs](https://support.ekohealth.com/hc/en-us/articles/13180195624347-Eko-App-FAQ), it states that "The software [EAS] analyzes simultaneous ECG and heart sounds. The software detects the presence of atrial fibrillation and normal sinus rhythm from the ECG signal."  There does not seem to be any indication that the software is generating new data, but rather is classifying the data points into distinct categories.  It is also important to note that the FDA has not cleared any generative AI devices to-date.  

# Black box AI vs. glass box AI
So with all this discussion on discriminative vs. generative models, why does this matter in practice?  AI systems as black boxes are nothing new, as I first encountered this during my time at IBM Watson.  Fast forward a decade, and the algorithms powering today's AI systems are far more advanced and not easily understood, even by the researchers who developed them.  

Much of this advancement was due to the "deep learning revolution" in 2012.  This Ars Technica [article](https://arstechnica.com/science/2019/12/how-neural-networks-work-and-why-theyve-become-a-big-business/?comments=1&comments-page=1) gives a nice summary of the history of neural networks.  While simple (single-layer) neural networks were first developed in the early 1950s, it started to see more mainstream success from 2012 onwards.  As the article highlights, the main contributors to its success were a combination of:

- Deeper networks (more layers)
- Large datasets (more data)
- Powerful compute (more processing efficiency)

Why it matters is that these systems can now model much more complex relationships on their own.  On the one hand, this is much more efficient than explicit coding by a human being; on the other, it is hard to decipher the reasoning why the system has made the decisions it did throughout the network.

Some argue that this is a problem in healthcare where increasingly sophisticated AI (specifically deep learning) systems pose a [risk](https://www.techtarget.com/healthtechanalytics/feature/Navigating-the-black-box-AI-debate-in-healthcare) for patient care.  If an AI model makes a diagnosis, wouldn't a physician want to know how it came to that conclusion?  There appears to be some consensus that in low-stakes use cases, such as administrative tasks, black box AI is not much of an issue.  However, there are high-stakes for clinical decision making, where accountability and liability are not well-defined (who gets sued when AI makes a wrong diagnosis?).  As a result, the lack of transparency may hinder more widespread adoption of healthcare AI.

Healthcare would be better served with [glass box AI](https://theconversation.com/what-is-a-black-box-a-computer-scientist-explains-what-it-means-when-the-inner-workings-of-ais-are-hidden-203888), where the algorithms, training data, and the model are open to the public.  However, this poses challenges for companies who are looking to protect their AI IP.  Fortunately, there is considerable development around explainable AI to help mitigate these risks.

## Conclusion

I appreciated this insightful and hopeful article for the future of healthcare.  Perhaps with this new wave of AI, there is a renewed commitment by people and processes to enable a better healthcare experience for everyone.  Finally, may we put that old fax machine to rest.












To see why this is, let's take a look at the [intricacies](https://spectrum.ieee.org/what-is-deep-learning) involved in a neural network with many layers:

![nn](/assets/images/2024-06/nn.png){:.centered}

Imagine trying to unravel all of this and explaining it to a non-technical person, let alone someone in a completely different field like healthcare.