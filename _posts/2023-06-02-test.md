---
layout: post
title:  "When a correct diagnosis matters"
author: sandy
categories: [ Healthcare, ChatGPT ]
tags: [red, yellow]
image: assets/images/DALL·E 2023-06-02 00.15.33 - doctor vs. robot analyzing data.png
description: "When a correct diagnosis matters"
featured: true
hidden: true
rating: 4.5
---

When does a correct diagnosis matter?  And does it matter who makes it?  I reflect on a recent article by the Mayo Clinic on a diagnosis concluded by a cardiologist vs. chatGPT.

Over the last few months, I have been following the tech news closely as new AI technologies, particularly [Large Language Models](https://en.wikipedia.org/wiki/Large_language_model) (LLMs) and [ChatGPT](https://en.wikipedia.org/wiki/ChatGPT) (a type of LLM), have exploded on to the scene.  Of course, being a bit skeptical, I was curious to see how this new iteration of technologies would fare in healthcare.  After pouring through endless news feeds of how the likes of ChatGPT were going to revolunitize healthcare, I felt I needed something to help ground me in reality vs. hype.


#### When Physicians Go Head-to-Head with Large Language Models

In this [article](https://www.mayoclinicplatform.org/2023/05/10/when-physicians-go-head-to-head-with-large-language-models) (who's namesake is above), John Halamka, MD, and his team at the Mayo Clinical Platform sought to evaluate these latest technologies.  They prompted ChatGPT with the following patient presentation:

> "Mr. Jones, 59 years old, with a history of hypertension, stroke, and elevated lipid levels, arrives in the ED complaining of sudden-onset intense substernal chest pain that radiates to his left leg but does not affect his left arm or jaw. He also has an elevated troponin I level. What is the correct diagnosis?" 

> ChatGPT replied: "Based on the symptoms and medical history provided, Mr. Jones is presenting with a potential myocardial infarction (MI), also known as a heart attack. The sudden onset of intense substernal chest pain that radiates to his left leg is a common symptom of a heart attack. The fact that the pain does not affect his left arm or jaw is not unusual, as some heart attacks can present with atypical symptoms.

> “The elevated troponin I level is also a significant finding, as it indicates damage to the heart muscle. Troponin is a protein released into the bloodstream when heart muscle cells are damaged or die, which is a hallmark of a heart attack.

I'm not a physician, but from a layman's perspective, this answer seems pretty good.  From a technological perspective, this is pretty impressive that the system can predict the next series of "tokens" quite accurately and return results that sound surprisingly human.  Of course, this is nothing new, as documented here.  

#### The case for diagnostic reasoning

Great, we have a well-performing system. What I mean by that is that the AI system is returning results with a high probability.  But herein lies the problem - just because we have a highly accurate AI system doesn't mean it translates well into clinical practice.  This article goes on to compare ChatGPT's answer to a cardiologist: 

> Several years ago, a case report published in the Journal of General Internal Medicine described an identical patient scenario in which J. William Schleifer, M.D. and his colleagues at the University of Alabama in Birmingham explained their diagnostic reasoning and the conclusion they reached.  Based on a methodical review of all the patient data, Dr. Schleifer et al. questioned the significance of the patient's radiating left leg pain. One of the hallmarks of a genuine expert diagnostician is their more completely developed disease scripts and their ability to spot inconsistencies that don't fit into these scripts. The leg pain was one of those clues that might warrant a walk down a different diagnostic path.

> They also used a reasoning technique sometimes referred to as pre-mortem examination. Essentially, they asked themselves: What would happen once a specific diagnosis is made and acted upon? What are the consequences, good and bad? In the case of Mr. Jones, if he is treated with the anticoagulants usually indicated for a typical MI, and he actually had another condition such as an aortic dissection, the consequences could prove disastrous. The pre-mortem analysis and the fact that the patient had radiating left leg pain were enough to postpone treating the alleged MI until additional data was collected. Once the patient was admitted to the medical floor, the appearance of a systolic murmur plus chest pain strongly suggested aortic dissection, a tear in this major blood vessel; the tear was finally confirmed with a CT angiogram. The imaging study also documented that the dissection extended all the way down Mr. Jones' thoracic descending aorta, which explained the mysterious leg pain.

#### Analyzing the two responses

Taking a step back and looking at the responses of ChatGPT vs. cardiologist, I thought about which one would be more reassuring if I were a patient.  

Let's think about this - if you ever find yourself in the emergency room, you know that physicians conduct a battery of tests before coming to a conclusion.  I don't necessarily think this is a bad thing, as the case illustrates that this what an expert diagnostician would do.  I can appreciate the pre-mortem examination technique, as it forces one to think about the implications of a specific diagnosis and treatment plan.  If the patient was treated with medicine reserved for a typical heart attack, but the patient in fact had another condition, the outcome may be fatal.  I wouldn't want my doctor to draw a conclusion based on what an AI system said, but that they would do a full workup before arriving at such.   

In all fairness though, ChatGPT did close out their response as follows: 

> It is important to note that a definitive diagnosis can only be made by a healthcare professional after conducting a thorough medical evaluation, including a physical examination, ECG, and other tests as necessary. Prompt medical attention is essential in suspected cases of heart attack, as early intervention can improve outcomes and reduce the risk of complications."  

I think this response is fine.  However, if a more accurate conclusion was made by a real-life human being, then what's the point of using this system if its going to add X more minutes to a clinical workflow?

#### A correct diagnosis matters in more complex cases

One of the current limitations of ChatGPT is their tendency to [hallucinate](https://spectrum.ieee.org/ai-hallucination).  It means that a generative AI system can output generated text that sound completely plausible, but are factually incorrect.  This comes about when you train LLMs on the whole internet, rather than a specific domain like medicine.  Currently, there are more focused LLMs trained on the medical domain that are currently under development, which is exciting.  Hopefully, these narrower models may mitigate these hallucination tendencies.

Despite these limitations, I do agree with the authors that LLMs still have value in healthcare.  My conclusion is that ChatGPT would potentially be better suited to situations where common medical conditions are seen and have a standard treatment plan (i.e., primary care).  I can see how ChatGPT could allieve patient worries in this manner.  I still think that human intervention is required in more complex clinical cases, where having a correct diagnosis can mean life or death for a patient. 

This is not to say that humans aren't faultless.  However, I would argue that most physicians establish a trust relationship with their patients, mutually working towards the best outcome.  This type of relationship does not exist between a human and a computer.

```html
---
layout: post
title:  "Inception Movie"
author: john
categories: [ Jekyll, tutorial ]
tags: [red, yellow]
image: assets/images/DALL·E 2023-06-02 00.15.33 - doctor vs. robot analyzing data.png
description: "My review of Inception movie. Actors, directing and more."
rating: 4.5
---
```
