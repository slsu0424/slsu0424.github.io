---
layout: post
title:  "NLP: Encoding sample PubMed abstracts using One-Hot Vectors"
author: sandy
categories: [ PubMed, NLP, tutorial ]
image: assets/images/vector.png
---

Last month, I gave a [talk](https://www.meetup.com/new-jersey-sql-data-platform-user-group/events/294231326/) at a local meetup group 
on LLMs, LangChain & SQL.  Since then, I've wanted to dig a little deeper into the technical foundations of LLMs, particularly neural networks.

This article will begin a series of technical tutorials that are geared towards healthcare care use cases.  While there is no shortage of resources to learn about LLMs, neural networks, or the latest ML algorithms, my focus will be to reinforce technical concepts I am not as familiar with.


## A simple approach to NLP - one-hot encoding

To start, I wanted to backtrack the origins of LLMs, which are a type of neural network.  A neural network is a type of machine learning approach that has been shown to perform well on NLP tasks.  For a great overview of NLP, check out this [guide](https://www.deeplearning.ai/resources/natural-language-processing).

The goal of NLP is to enable computers to "understand" natural language in order to perform some task, such as sentiment analysis.  In order to do so, natural language (text format) has to be converted (encode) into a numerical format.

There are numerous approaches to encode text, with more advanced approaches surfacing over the years.  I will start with one-hot encoding, as it is one of the most familiar data pre-processing techniques for ML.  However, we will also begin to see the limitations of such a technique through the example below. 

For NLP, one of the simplest techniques for encoding text is to represent a categorical variable (word) as a binary vector.  The index of the word is marked with '1', and all other words are marked with '0'.

Consider the following text: 'I feel sick' -> [1, 0, 0]

One Hot encoding is a representation of categorical variables as binary vectors. Each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1. Here is a One Hot vector representation:

 To represent each word, you will create a zero vector with length equal to the vocabulary, then place a one in the index that corresponds to the word

## Let's convert a few PubMed abstracts to one-hot vectors

To put the above tutorial into practice, I thought I would give this a try with a few abstracts.  Let's say that we want to perform an NLP task on papers that discuss cardiovascular disease (CVD) risk factors.  This is what my query and results look like in PubMed:

![PubMed](/assets/images/2023-07-28_PubMed.png)

Each abstract represents a single document in the corpus.  Next, we're going to take the first 5 words from each abstract.  For simplicity purposes, we will ignore header terms such as 'Context', 'Introduction', 'Purpose'.

>Doc 1: "The difference between actual and"  
Doc 2: "Cardiovascular disease (CVD) is a"  
Doc 3: "To evaluate awareness about cardiovascular"   

Next, we need to find the unique words within this corpus.  We have 14 unique words:

>Doc 1: The(1) difference(2) between(3) actual(4) and(5)  
Doc 2: Cardiovascular(6) disease(7) (CVD)(8) is(9) a(10)  
Doc 3: To(11) evaluate(12) awareness(13) about(14) cardiovascular(6)  

We represent these unique words as our vocabulary:
    
>Vocabulary = {'The', 'difference', 'between', 'actual', 'and', â€˜Cardiovascular', 'disease', '(CVD)', 'is', 'a', 'To', 'evaluate', 'awareness', 'about'}

Each word is represented as a one-hot vector with a length equal to the size of the vocabulary (N = 14).  That is, each word in a document is represented with a '1' corresponding to the unique word, and '0' representing all other words.  We will have something that looks like this:

![encoding](/assets/images/2023-08-02_encoding.png)

## Limitations of one-hot encoding

One of the first challenges seen with encoding words as one-hot vectors is that no information can be gleaned from the words they represent, or how the words relate to each other.  The one-hot encoding technique tells us nothing about the similarities between words.  If we were to graph each word in a 14-dimensional space and calculate the Euclidean distance between each vector, we would see that... 


Another challenge is sparse data.  When each word in a document is replaced with a one-hot vector, we start to get a large space with a lot of zeroes.  In fact, when increase our vocabulary size (for example, double the vocabulary size N = 28 words), our word representation will morph into something like this:

![encoding_sparse](/assets/images/2023-08-02_encoding_sparse.png)


This representation is:
- sparse
- highly-dimensional
- hard-coded
  
which makes it difficult for a machine to detect any sort of meaningful patterns.


## Conclusion

In closing, we demonstrated 

## References
+ <https://www.deeplearning.ai/resources/natural-language-processing>
+ <https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf>
+ <https://developers.google.com/machine-learning/glossary#one-hot-encoding>
+ <https://developers.google.com/machine-learning/crash-course/representation/feature-engineering>
+ <https://www.tensorflow.org/text/guide/word_embeddings>
+ <https://medium.com/intelligentmachines/word-embedding-and-one-hot-encoding-ad17b4bbe111>

## Writing code blocks

+ ~~strike through~~
+ ==highlight==
+ \*escaped characters\*


## Writing code blocks

There are two types of code elements which can be inserted in Markdown, the first is inline, and the other is block. Inline code is formatted by wrapping any word or words in back-ticks, `like this`. Larger snippets of code can be displayed across multiple lines using triple back ticks:

```
.my-link {
    text-decoration: underline;
}
```

#### HTML

```html
<li class="ml-1 mr-1">
    <a target="_blank" href="#">
    <i class="fab fa-twitter"></i>
    </a>
</li>
```

#### CSS

```css
.highlight .c {
    color: #999988;
    font-style: italic; 
}
.highlight .err {
    color: #a61717;
    background-color: #e3d2d2; 
}
```

#### JS

```js
// alertbar later
$(document).scroll(function () {
    var y = $(this).scrollTop();
    if (y > 280) {
        $('.alertbar').fadeIn();
    } else {
        $('.alertbar').fadeOut();
    }
});
```

#### Python

```python
print("Hello World")
```

#### Ruby

```ruby
require 'redcarpet'
markdown = Redcarpet.new("Hello World!")
puts markdown.to_html
```

#### C

```c
printf("Hello World");
```




![walking]({{ site.baseurl }}/assets/images/8.jpg)

## Reference lists

The quick brown jumped over the lazy.

Another way to insert links in markdown is using reference lists. You might want to use this style of linking to cite reference material in a Wikipedia-style. All of the links are listed at the end of the document, so you can maintain full separation between content and its source or reference.

## Full HTML

Perhaps the best part of Markdown is that you're never limited to just Markdown. You can write HTML directly in the Markdown editor and it will just work as HTML usually does. No limits! Here's a standard YouTube embed code as an example:

<p><iframe style="width:100%;" height="315" src="https://www.youtube.com/embed/Cniqsc9QfDo?rel=0&amp;showinfo=0" frameborder="0" allowfullscreen></iframe></p>
