---
layout: post
title:  "NLP: Encoding sample PubMed abstracts using One-Hot Vectors"
author: sandy
categories: [ PubMed, NLP, tutorial ]
image: assets/images/2023-08-02_fever.png
---

Last month, I gave a [talk](https://www.meetup.com/new-jersey-sql-data-platform-user-group/events/294231326/) at a local meetup group 
on LLMs, LangChain & SQL.  Since then, I've wanted to dig a little deeper into the technical foundations of LLMs, particularly neural networks.

This article will begin a series of technical tutorials that are geared towards healthcare care use cases.  While there is no shortage of resources to learn about LLMs, neural networks, or the latest ML algorithms, my focus will be to reinforce technical concepts I am not as familiar with.


## A simple approach to NLP - one-hot encoding

To start, I wanted to backtrack the origins of LLMs, which are a type of neural network.  A neural network is a type of machine learning approach that has been shown to perform well on NLP tasks.  For a great overview of NLP, check out this [guide](https://www.deeplearning.ai/resources/natural-language-processing).

The goal of NLP is to enable computers to "understand" natural language in order to perform some task, such as sentiment analysis.  In order to do so, natural language (text format) has to be converted (encode) into a numerical format.

There are numerous approaches to encode text, with more advanced approaches surfacing over the years.  I will start with one-hot encoding, as it is one of the most familiar data pre-processing techniques for ML.  However, we will also begin to see the limitations of such a technique through the example below. 

For NLP, one of the simplest techniques for encoding text is to represent each categorical variable (a word) as a binary vector.  A one-hot vector is a type of binary vector where the index of the word is denoted as '1', and all other words are denoted as '0'.  The size/dimension of the vector is determined by the total number of unique words (vocabulary) found in the text.

Consider the following text: 'I have a fever'.  The vocabulary consists of 4 unique words (I, have, a, fever), and each word is represented as a one-hot vector.

![PubMed](/assets/images/2023-08-02_fever.png)


## Let's convert a few PubMed abstracts to one-hot vectors

To put the above tutorial into practice, I thought I would give this a try with a few abstracts.  Let's say that we want to perform an NLP task on papers that discuss cardiovascular disease (CVD) risk factors.  This is what my query and results look like in PubMed:

![PubMed](/assets/images/2023-07-28_PubMed.png)

Each abstract represents a single document in the corpus.  Next, we're going to take the first 5 words from each abstract.  For simplicity purposes, we will ignore header terms such as 'Context', 'Introduction', 'Purpose'.

>Doc 1: "The difference between actual and"  
Doc 2: "Cardiovascular disease (CVD) is a"  
Doc 3: "To evaluate awareness about cardiovascular"   

Next, we need to find the unique words within this corpus.  We have 14 unique words:

>Doc 1: The(1) difference(2) between(3) actual(4) and(5)  
Doc 2: Cardiovascular(6) disease(7) (CVD)(8) is(9) a(10)  
Doc 3: To(11) evaluate(12) awareness(13) about(14) cardiovascular(6)  

We represent these unique words as our vocabulary:
    
>Vocabulary = {'The', 'difference', 'between', 'actual', 'and', â€˜Cardiovascular', 'disease', '(CVD)', 'is', 'a', 'To', 'evaluate', 'awareness', 'about'}

Each word is represented as a one-hot vector with a length equal to the size of the vocabulary (N = 14).  That is, each word in a document is represented with a '1' corresponding to the unique word, and '0' representing all other words.  We will have something that looks like this:

![encoding](/assets/images/2023-08-02_encoding.png)

## Limitations of one-hot encoding

One challenge seen with the one-hot encoding technique is that there is no information about the words the vectors represent, or how the words relate to each other.  In other words, it tells us nothing about the similarities between words.  If we were to graph each vector and calculate the distance (e.g., [Hamming distance](<https://machinelearningmastery.com/distance-measures-for-machine-learning/>)) between them, we would see that the difference between any two vectors is the same.


Another challenge is sparse data.  When each word in a document is replaced with a one-hot vector, we start to get a large space with a lot of zeroes.  In fact, when increase our vocabulary size (for example, double the vocabulary size N = 28 words), our word representation will morph into something like this:

![encoding_sparse](/assets/images/2023-08-02_encoding_sparse.png)


This representation is:
- sparse (majority of the elements are zeroes)
- highly-dimensional (a larger feature space requires more computational power and data)
- hard-coded (machine does not learning anything from the data)
  
Combined, this technique makes it difficult for a machine to detect any sort of meaningful patterns to provide a useful prediction.


## Conclusion

In this tutorial, we considered the one-hot encoding technique, a very simple way to encode categorical variables for natural language processing tasks.  By taking a subset of PubMed abstracts that discussed cardiovascular disease (CVD) risk factors, I was able to demonstrate how this approach becomes highly inefficient as larger vocabularies are considered.  This is due to , due to the  can lead to an increasingly large and sparse feature space, which would require more computational power and data to provide any meaningful insight.

There have been additional approaches that have improved upon the limitations of one-hot encoding and have worked increasingly well with neural networks.  I will explore these in future posts.



## References
+ <https://www.deeplearning.ai/resources/natural-language-processing>
+ <https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf>
+ <https://developers.google.com/machine-learning/glossary#one-hot-encoding>
+ <https://developers.google.com/machine-learning/crash-course/representation/feature-engineering>
+ <https://www.tensorflow.org/text/guide/word_embeddings>
+ <https://medium.com/intelligentmachines/word-embedding-and-one-hot-encoding-ad17b4bbe111>
+ <https://machinelearningmastery.com/distance-measures-for-machine-learning/>
+ <https://builtin.com/data-science/curse-dimensionality>

## Writing code blocks

+ ~~strike through~~
+ ==highlight==
+ \*escaped characters\*


## Writing code blocks

There are two types of code elements which can be inserted in Markdown, the first is inline, and the other is block. Inline code is formatted by wrapping any word or words in back-ticks, `like this`. Larger snippets of code can be displayed across multiple lines using triple back ticks:

```
.my-link {
    text-decoration: underline;
}
```

#### HTML

```html
<li class="ml-1 mr-1">
    <a target="_blank" href="#">
    <i class="fab fa-twitter"></i>
    </a>
</li>
```

#### CSS

```css
.highlight .c {
    color: #999988;
    font-style: italic; 
}
.highlight .err {
    color: #a61717;
    background-color: #e3d2d2; 
}
```

#### JS

```js
// alertbar later
$(document).scroll(function () {
    var y = $(this).scrollTop();
    if (y > 280) {
        $('.alertbar').fadeIn();
    } else {
        $('.alertbar').fadeOut();
    }
});
```

#### Python

```python
print("Hello World")
```

#### Ruby

```ruby
require 'redcarpet'
markdown = Redcarpet.new("Hello World!")
puts markdown.to_html
```

#### C

```c
printf("Hello World");
```




![walking]({{ site.baseurl }}/assets/images/8.jpg)

## Reference lists

The quick brown jumped over the lazy.

Another way to insert links in markdown is using reference lists. You might want to use this style of linking to cite reference material in a Wikipedia-style. All of the links are listed at the end of the document, so you can maintain full separation between content and its source or reference.

## Full HTML

Perhaps the best part of Markdown is that you're never limited to just Markdown. You can write HTML directly in the Markdown editor and it will just work as HTML usually does. No limits! Here's a standard YouTube embed code as an example:

<p><iframe style="width:100%;" height="315" src="https://www.youtube.com/embed/Cniqsc9QfDo?rel=0&amp;showinfo=0" frameborder="0" allowfullscreen></iframe></p>
