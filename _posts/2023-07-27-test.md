---
layout: post
title:  "NLP: Encoding sample PubMed abstracts using One-Hot Vectors"
author: sandy
categories: [ PubMed, NLP, tutorial ]
image: assets/images/vector.png
---

Last month, I gave a [talk](https://www.meetup.com/new-jersey-sql-data-platform-user-group/events/294231326/) at a local meetup group 
on LLMs, LangChain & SQL.  Since then, I've wanted to dig a little deeper into the technical foundations of LLMs, particularly neural networks.

This article will begin a series of technical tutorials that are geared towards healthcare care use cases.  While there is no shortage of resources to learn about LLMs, neural networks, or the latest ML algorithms, my focus will be to reinforce technical concepts I am not as familiar with.


## A simple approach to NLP - one-hot encoding

To start, I wanted to backtrack the origins of LLMs, which are a type of neural network.  A neural network is a type of machine learning approach that has been shown to perform well on NLP tasks.  For a great overview of NLP, check out this [guide](https://www.deeplearning.ai/resources/natural-language-processing).

The goal of NLP is to enable computers to "understand" natural language in order to perform some task, such as sentiment analysis.  In order to do so, natural language (text format) needs to be converted (encode) into a numerical format.

There are numerous approaches to encode text, or words.  Although more advanced techniques have surfaced over the years, I will start with the concept of one-hot encoding as it is one of the most familiar techniques for ML.  However, we will also begin to see the limitations of such a technique through the example below. 

For NLP, one of the simplest approaches for encoding is to generate a one-hot vector (1 x N matrix, where N is the size of the vocabulary) that will distinguish a word from every other word in a vocabulary.  This [tutorial](https://gyan-mittal.com/nlp-ai-ml/nlp-word-encoding-by-one-hot-encoding/) does a really good job explaining the logic behind this. 

## Let's convert a few PubMed abstracts to vectors

To put the above tutorial into practice, I thought I would give this a try with a few abstracts.  Let's say that we want to perform an NLP task on papers that discuss cardiovascular disease (CVD) risk factors.  This is what my query and results look like in PubMed:

![PubMed](/assets/images/2023-07-28_PubMed.png)

Each abstract represents a single document in the corpus.  Next, we're going to take the first 10 words from each abstract.  For simplicity purposes, we will ignore header terms such as 'Context', 'Introduction', 'Purpose'.

&nbsp;&nbsp;Doc 1: "The difference between actual and perceived risk levels shows distorted"  

    Doc 2: "Cardiovascular disease (CVD) is a major cause of death in"  

    Doc 3: "To evaluate awareness about cardiovascular (CVD) risk among a racially"   

Next, we need to find the unique words within this corpus.  We have 26 unique words, and each () represents the frequency of the word:

    Doc 1: The(1) difference(1) between(1) actual(1) and(1) perceived(1) risk(2) levels(1) shows(1) distorted(1)  

    Doc 2: Cardiovascular(2) disease(1) (CVD)(2) is(1) a(2) major(1) cause(1) of(1) death(1) in(1)  

    Doc 3: To(1) evaluate(1) awareness(1) about(1) cardiovascular(2) (CVD)(2) risk(2) among(1) a(2) racially(1)

Next, we represent each document as a dictionary.  It starts with an index of 0 all the way up to 26 (last word):
    
    {‘i’: 0, ‘The’: 1, ‘difference’: 2, ‘between’: 3, ‘actual’: 4, ‘and’: 5, ‘perceived’: 6, ‘risk’: 7, ‘levels’: 8, ‘shows’: 9, ‘distorted’: 10}...

The documents can be represented as follows in terms of a word array: \
    Doc 1: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  
    Doc 2: [4, 1, 2, 5]  
    Doc 3:


Now, each word has to be represented as a one-hot vector of the dimension 26 (vocabulary size).  That is, each word in a document is represented as a single line with 



## References
+ <https://www.deeplearning.ai/resources/natural-language-processing>
+ <https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf>

## Writing code blocks

+ ~~strike through~~
+ ==highlight==
+ \*escaped characters\*


## Writing code blocks

There are two types of code elements which can be inserted in Markdown, the first is inline, and the other is block. Inline code is formatted by wrapping any word or words in back-ticks, `like this`. Larger snippets of code can be displayed across multiple lines using triple back ticks:

```
.my-link {
    text-decoration: underline;
}
```

#### HTML

```html
<li class="ml-1 mr-1">
    <a target="_blank" href="#">
    <i class="fab fa-twitter"></i>
    </a>
</li>
```

#### CSS

```css
.highlight .c {
    color: #999988;
    font-style: italic; 
}
.highlight .err {
    color: #a61717;
    background-color: #e3d2d2; 
}
```

#### JS

```js
// alertbar later
$(document).scroll(function () {
    var y = $(this).scrollTop();
    if (y > 280) {
        $('.alertbar').fadeIn();
    } else {
        $('.alertbar').fadeOut();
    }
});
```

#### Python

```python
print("Hello World")
```

#### Ruby

```ruby
require 'redcarpet'
markdown = Redcarpet.new("Hello World!")
puts markdown.to_html
```

#### C

```c
printf("Hello World");
```




![walking]({{ site.baseurl }}/assets/images/8.jpg)

## Reference lists

The quick brown jumped over the lazy.

Another way to insert links in markdown is using reference lists. You might want to use this style of linking to cite reference material in a Wikipedia-style. All of the links are listed at the end of the document, so you can maintain full separation between content and its source or reference.

## Full HTML

Perhaps the best part of Markdown is that you're never limited to just Markdown. You can write HTML directly in the Markdown editor and it will just work as HTML usually does. No limits! Here's a standard YouTube embed code as an example:

<p><iframe style="width:100%;" height="315" src="https://www.youtube.com/embed/Cniqsc9QfDo?rel=0&amp;showinfo=0" frameborder="0" allowfullscreen></iframe></p>
