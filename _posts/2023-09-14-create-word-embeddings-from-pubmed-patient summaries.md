---
layout: post
title:  "Create word embeddings from PubMed patient summaries"
author: sandy
categories: [ PubMed, NLP, tutorial, SQL, Azure ]
image: assets/images/2023-09/OIP_resize.jpg
---
Building upon the previous [tutorial](https://slsu0424.github.io/encoding-pubmed-abstracts-for-nlp-tasks/) on one-hot encoding, this tutorial will review the concept of word embeddings and apply this to real-life data.  

For our example, we will extract patient summaries from PubMed.  From these patient summaries, we will label those that had COVID-19 and those that did not.  Selected sentences are used to create word embeddings.  These embeddings are trained as part a neural network model to perform a classification task.  By training the word embeddings, the computer will learn if there are any meaningful relationships between the words in the text. 

## A short intro to Word Embeddings

Word Embeddings were a bit of a complex concept to grasp, until I got into the weeds of building one.  I've come to appreciate that they are an important concept in deep learning, for the reason that word meanings can be approximated mathematically.  

To best understand the intuition behind word embeddings, I highly recommend reading this [blog post](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations) from Christopher Olah.

There are a number of techniques available to build a word embedding, including using pre-trained embeddings generated by GloVe, word2vec, etc.  In this tutorial, we will train an embedding from scratch.   

## Let's get data

I selected a dataset of ~167K PubMed patient summaries via [HuggingFace](https://huggingface.co/datasets/zhengyun21/PMC-Patients/tree/main).  To demonstrate a low-code approach, SQL Server and Azure Data Studio will be used.

The data is loaded into [SQL Server on a Mac](https://builtin.com/software-engineering-perspectives/sql-server-management-studio-mac).  Start SQL Server via the terminal, giving the username and password:

```
$ mssql -u <sql server username> -p <sql server password>
```

We then [connect](https://www.sqlshack.com/sql-server-data-import-using-azure-data-studio/) SQL Server to Azure Data Studio, where we can access and query the data.

Attached is a screenshot to modify the columns before importing the data:

![AzureDataStudio](/assets/images/2023-09/azstudio_setup3.png)

Run the below [query](https://github.com/slsu0424/pmc-patients/blob/develop/pmc-patients.sql) to return the first 100 records.

```
SELECT TOP (100) [patient_id]
  ,[patient_uid]
  ,[PMID]
  ,[file_path]
  ,[title]
  ,[patient]
  ,[age]
  ,[gender]
  ,[similar_patients]
  ,[relevant_articles]
FROM [test].[dbo].[PMC-Patients]
```

## Define class labels

Since this is a binary classification task, we will label the dataset as:
- patients with COVID-19 = '1'
- patients without COVID-19 = '0'

```python
labels = []

df['label'] = ''

for index, row in df.iterrows():
    if 'COVID-19' in row['patient']:
        row['label'] = '1'
    else:
        row['label'] = '0' 

    labels.append(row['label'])

# convert list to array
labels_arr = np.array(labels).astype(float)

print(labels_arr)
```

```
[1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.
 0. 0. 1. 0.]
```

## Create a corpus

Now that we have our labeled dataset, we can create a corpus.  We take the 1st sentence from each document.  A sample of the first 3 documents in the corpus is below:

```
['This 60-year-old male was hospitalized due to moderate ARDS from COVID-19 with symptoms of fever, dry cough, and dyspnea.', 
'We describe the case of a 55-year-old male who presented to the emergency department via emergency medical services for the chief complaint of sudden onset shortness of breath that woke him from his sleep just prior to arrival.', 
'A 20-year-old Caucasian male (1.75 m tall and 76 kg (BMI 24.8)), was admitted to the medical department for persistent hyperpyrexia, severe sore throat, dyspnea, and impaired consciousness with stupor., 
...]
```

For 100 documents, there are 2451 total words in the corpus.


## Convert text to integers

Since we saw the limitations with one-hot encoding, a better approach would be to assign each word a unique integer.  The integer encoding for a specific word remains the same across all documents, so this will help reduce the size of the corpus to unique words only. 

To do this, Keras (neural network library) provides a handy **Tokenizer() API** that can handle multiple documents.  For a deeper understanding of how to implement this, see this [tutorial](https://machinelearningmastery.com/prepare-text-data-deep-learning-keras).

```python
encod_corp = []

# fit tokenizer on docs
t = Tokenizer()
t.fit_on_texts(corp) 
encod_corp = t.texts_to_sequences(corp) # integer encode docs

# get unique words
vocab = t.word_index

print("vocab:")
for i,v in enumerate(vocab, 1):
   print(i,v)

vocab_size = len(vocab) # input into embedding layer
print('Vocab size = %s unique words' % vocab_size)
```
```
vocab:
1 a
2 of
3 with
4 and
5 the

Vocab size = 932 unique words
``` 
Out of 2451 total words, 932 unique words are found.

## Pad the documents

Keras requires that all documents must be the same length.  We find the maximum length of a document, which is 55 words.  Zeroes are then added to the shorter documents using the **pad_sequences** function.

```python
# pad the docs with zeros
pad_corp=pad_sequences(encod_corp,maxlen=maxlen,padding='post',value=0.0)

print(pad_corp)
```

```
[[ 31 281  10   7 160  44   6 282 283  28  54   3  84   2  29  45  36   4
  161   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0]
   ... 
```

The above array represents the text of Document 1.  A sample mapping of the array to words is below:

| This | 60-year-old | male | was | hospitalized | due | to | moderate | ARDS | from | COVID-19 | ...
|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|
| 31 | 281 | 10 | 7 | 160 | 44 | 6 |282 | 283 | 28 | 54 |...|

## Create an embedding 

To create the embedding, we create a Keras Sequential model.  Sequential means that each layer in the network has exactly one input and one output.  To define the embedding, we need 3 inputs:

- input_dim: size of vocabulary
- output_dim: number of dimensions the word should be embedded into
- input_length: maximum length of a document

The output_dim is the size of the output vectors for each word.  For example, a output_dim = 2 means that every word is mapped to a vector with 2 elements, or features.  These numbers can be chosen arbitrarily.  A larger output_dim will have more features to train on, but also more computationally expensive.  

*Note:* I played around with the output_dim in multiples of 2 (2 to 32), and did not see a difference in accuracy when training the embedding.


```python
# create keras model
model = Sequential()

# define embedding
embedding_layer = Embedding(
                        input_dim=vocab_size+1,
                        output_dim=2, 
                        input_length=maxlen)

# add layers
model.add(embedding_layer) # layer 0
model.add(Flatten()) # layer 1
model.add(Dense(1, activation='sigmoid'))  # layer 2

# configure the learning process
model.compile(
        optimizer='adam',
        loss='categorical_crossentropy', 
        metrics=['accuracy'])

# model prediction
embedding_output = model.predict(pad_corp)
```

## Visualize intial embeddings

The output of an embedding layer is a lookup table, which maps each word in the vocabulary to a set of random numbers in the dimension specified.  These numbers are initialized randomly before training the model.

```python
# extract embedding
embedding_layer = model.get_layer(index=0)

embedding_matrix = embedding_layer.get_weights()[0]
```

For example, since we set our output_dim = 2, we should expect to see each word mapped to 2 random numbers:

```
[[ 3.51696648e-02 -4.76405397e-02] --> 'a'
 [-1.02797039e-02  1.60155557e-02] --> 'of'
 [ 3.44626047e-02 -2.51591206e-02] --> 'with'
 [ 2.46063583e-02  2.83009149e-02] --> 'and'
 [ 4.52322103e-02  2.24443115e-02] --> 'the'
 [ 2.98631825e-02  3.14567201e-02] --> 'to'
 ...
```

Revisiting the first document, we see that each value from the embedding layer is mapped to a word in that document:

```
| This | 60-year-old | male | was | hospitalized | due | to | moderate | ARDS | from | COVID-19 


[[[ 6.34747744e-03  1.50704272e-02] --> 'This'
  [-1.62608549e-03  3.41557600e-02]
  [ 2.57278569e-02 -2.51035690e-02]
  [ 1.29629485e-02  3.39476801e-02]
  [-4.31921594e-02 -2.97903772e-02]
  [ 1.54621489e-02 -4.22002673e-02]
  [-1.35600083e-02  4.81962077e-02] --> 'to'
  [ 3.45612504e-02 -2.81577595e-02]
  [ 1.68705098e-02 -1.58230215e-03]
  [ 2.84120701e-02 -4.12044898e-02]
  [ 2.10472234e-02  3.22710983e-02]
  [ 2.46063583e-02  2.83009149e-02]
  [-4.78946567e-02 -2.18540188e-02]
  [ 3.44626047e-02 -2.51591206e-02]
  [-2.79271491e-02 -3.28389555e-02]
  [ 4.69936766e-02  1.71788000e-02]
  [-1.44309923e-03  4.02818359e-02]
  [ 4.52322103e-02  2.24443115e-02]
  [-3.90867107e-02 -3.23803648e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]
  [ 3.51696648e-02 -4.76405397e-02]]

```


Let's see how this looks visually.  Since these embeddings are not trained, it would make sense that the words are fairly scattered:

![](/assets/images/2023-09/output1.png)


## Visualize trained embeddings

After adding the embedding layer, we have a N dimension.  We need to compress that into a 2D vector.  

## Conclusion

In this tutorial, we explored one-hot encoding, a very simple way to convert categorical variables for natural language processing tasks.  By taking a subset of PubMed abstracts, we were able to see how this approach becomes highly inefficient with larger vocabularies.  Some of these inefficiencies are due to a lack of understanding relationships between words, as well as a sparse and highly-dimensional feature space.

We can boost the performance of the training accuracy by adding in another layer, such as a convolution layer.  I will explore these in future posts.


## References

Data/SQL Server:
+ <https://huggingface.co/datasets/zhengyun21/PMC-Patients/tree/main>
+ <https://builtin.com/software-engineering-perspectives/sql-server-management-studio-mac>
+ <https://www.sqlshack.com/sql-server-data-import-using-azure-data-studio/>