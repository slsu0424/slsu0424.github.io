---
layout: post
title:  "Create word embeddings from PubMed patient summaries"
author: sandy
categories: [ PubMed, NLP, tutorial, SQL, Azure ]
image: assets/images/2023-09/OIP_resize.jpg
---
Building upon the previous [tutorial](https://slsu0424.github.io/encoding-pubmed-abstracts-for-nlp-tasks/) on one-hot encoding, this tutorial will review the concept of word embeddings and apply this to real-life data.  

For our example, we will extract patient summaries (documents) from PubMed.  From these patient summaries, we will label those that had COVID-19, and those that did not.  Portions of text from each document will be selected to create word embeddings.  These embeddings will be "trained" as part a neural network model to perform a classification task.  By training the word embeddings, the computer will learn if there are any meaningful relationships between the words in the text. 

## A short intro to Word Embeddings

Word Embeddings were a bit of a complex concept to grasp, until I got into the weeds of building one.  I've come to learn they are an important concept in deep learning, for the very reason that semantic meaning of words can be approximated mathematically.  

To best understand the intuition behind word embeddings, I highly recommend reading this [blog post](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations) from Christopher Olah.

There are a number of techniques available to build a word embedding, including pre-trained embeddings generated by GloVe, word2vec.  For the purposes of this tutorial, we will train an embedding from scratch. 

As a quick note, LLMs use word embeddings, but the technique used to build them are proprietary.  

## Let's get data

I selected a dataset of ~167K PubMed patient summaries via [HuggingFace](https://huggingface.co/datasets/zhengyun21/PMC-Patients/tree/main).  To demonstrate a low-code approach, SQL Server and Azure Data Studio will be used.

The data is loaded into SQL Server on a Mac.  For further instructions on how to set this up, check out this [tutorial](https://builtin.com/software-engineering-perspectives/sql-server-management-studio-mac).  

SQL Server is started via the terminal giving the username and password (Docker must be run on your machine):

```
$ mssql -u <sql server username> -p <sql server password>
```

Azure Data Studio is used to access and query the data.  To connect to a SQL Server using Azure Data Studio, review this [tutorial](https://www.sqlshack.com/sql-server-data-import-using-azure-data-studio/).  

Attached is a screenshot to modify the columns before importing the data.

![AzureDataStudio](/assets/images/2023-09/azstudio_setup3.png)

We run the below [query](https://github.com/slsu0424/pmc-patients/blob/develop/pmc-patients.sql) to extract the first 100 records.

```
SELECT TOP (100) [patient_id]
  ,[patient_uid]
  ,[PMID]
  ,[file_path]
  ,[title]
  ,[patient]
  ,[age]
  ,[gender]
  ,[similar_patients]
  ,[relevant_articles]
FROM [test].[dbo].[PMC-Patients]
```

*Note:* While this represents a longer approach to loading and manipulating the dataset, you may wish to explore other approaches:
1. Load dataset into pandas dataframe.
2. Load dataset into a cloud database, such as Azure SQL Database.  Please keep in mind there are costs associated with running the database in the cloud, as well as querying costs.

## Define class labels

Since this is a binary classification task, we will label the dataset as:
- patients with COVID-19 = '1'
- patients without COVID-19 = '0'

```python
labels = []

df['label'] = ''

for index, row in df.iterrows():
    if 'COVID-19' in row['patient']:
        row['label'] = '1'
    else:
        row['label'] = '0' 

    labels.append(row['label'])

# convert list to array
labels_arr = np.array(labels).astype(float)

print(labels_arr)
```

Output:

```
[1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.
 0. 0. 1. 0.]
```

## Create a corpus

Now that we have our labeled dataset, the next step is to create a corpus.  We take the first 3 sentences from each document.  The resulting corpus is a python dictionary; a sample of the first 3 documents is below:

```
['This 60-year-old male was hospitalized due to moderate ARDS from COVID-19 with symptoms of fever, dry cough, and dyspnea. We encountered several difficulties during physical therapy on the acute ward.', 
'We describe the case of a 55-year-old male who presented to the emergency department via emergency medical services for the chief complaint of sudden onset shortness of breath that woke him from his sleep just prior to arrival.', 
'A 20-year-old Caucasian male (1.75 m tall and 76 kg (BMI 24.8)), was admitted to the medical department for persistent hyperpyrexia, severe sore throat, dyspnea, and impaired consciousness with stupor. Persistent symptoms started at home 4 days before and he assumed clarithromycin as empiric antibiotic therapy.', ...]
```

For 100 documents, there are X total words in the corpus.


## Convert text to integers

As explored in the previous tutorial, categorical variables (text) must be converted into numerical variables.  One approach would be one-hot encoding, but there are limitations with this technique.  A better approach would be to tag each word with a unique integer.  The nice thing about this is that the integer encoding for a specific word remains the same across all documents.  For example, ... 

To do this, Keras (a neural network library) provides a handy **Tokenizer() API** that can handle multiple documents.  For a deeper understanding of how to implement this, see this [tutorial](https://machinelearningmastery.com/prepare-text-data-deep-learning-keras).

```python
encod_corp = []

# fit tokenizer on docs
t = Tokenizer()
t.fit_on_texts(corp) 
encod_corp = t.texts_to_sequences(corp) # convert docs to num sequence

# get unique words
vocab = t.word_index

print("vocab:")
for i in vocab:
    print(i)

vocab_size = len(vocab) # input into embedding layer
print('Vocab size = %s unique words' % vocab_size)

```

Output:

```
vocab:
a
and
the
of
with
was
to
year
old
in
...

Vocab size = 1842 unique words

``` 
The size of the vocabulary (1842) will be important as an input for the embedding layer.

## Pad the documents

The next thing that Keras requires is that all documents must be of the same length.  We must find the maximum length of a document, and add padding (zeroes) to the shorter documents.  This is accomplished using the **pad_sequences** function:

```python
# pad the docs with zeros
pad_corp=pad_sequences(encod_corp,maxlen=maxlen,padding='post',value=0.0)

# show full numpy array

print(pad_corp)
```

## Create an embedding 

To create the embedding, we create a Keras Sequential model.  By sequential, this means that each layer in the network has exactly one input (tensor) and one output (tensor).  To define the embedding, we need 3 inputs:

- input_dim: size of vocabulary
- output_dim: how many dimensions the word should be embedded into
- input_length: maximum length of a document

I like to think of the output_dim as a form of compression.  That is, we want to compress each word such that its meaning can be represented by N number of dimensions. 

To better understand the logic behind creating an embedding, I found these [articles](https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work?rq=1) particulary helpful.  

```python
# create keras model
model = Sequential()

# define embedding
embedding_layer = Embedding(
                        input_dim=vocab_size+1,
                        output_dim=2, 
                        input_length=maxlen)

# add layers
model.add(embedding_layer) # layer 0
model.add(Flatten()) # layer 1
model.add(Dense(1, activation='sigmoid'))  # layer 2

# configure the learning process
model.compile(
        optimizer='adam',
        loss='categorical_crossentropy', 
        metrics=['accuracy'])

# model prediction
embedding_output = model.predict(pad_corp)

```

## Visualize intial embeddings

The output of an embedding layer is a lookup table, which maps each word in the vocabulary to a set of random numbers in the dimension specified.  For example, since we set our output_dim = 2, we should expect to see each word mapped to 2 random numbers:


When we look at a document, we will see that each embedding value is mapped to a word in that document:


Let's see how this looks visually.  Since these embeddings are not trained, it would make sense that the words are fairly scattered:




## Visualize trained embeddings


## Conclusion

In this tutorial, we explored one-hot encoding, a very simple way to convert categorical variables for natural language processing tasks.  By taking a subset of PubMed abstracts, we were able to see how this approach becomes highly inefficient with larger vocabularies.  Some of these inefficiencies are due to a lack of understanding relationships between words, as well as a sparse and highly-dimensional feature space.

We can boost the performance of the training accuracy by adding in another layer, such as a convolution layer.  I will explore these in future posts.


## References

Data/SQL Server:
+ <https://huggingface.co/datasets/zhengyun21/PMC-Patients/tree/main>
+ <https://builtin.com/software-engineering-perspectives/sql-server-management-studio-mac>
+ <https://www.sqlshack.com/sql-server-data-import-using-azure-data-studio/>