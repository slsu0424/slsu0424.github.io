---
layout: post
title:  "Create word embeddings from PubMed patient summaries"
author: sandy
categories: [ PubMed, NLP, tutorial, SQL, Azure ]
image: assets/images/2023-09/OIP_resize.jpg
---
Building upon the previous [tutorial](https://slsu0424.github.io/encoding-pubmed-abstracts-for-nlp-tasks/) on one-hot encoding, this tutorial will review the concept of word embeddings and apply this to real-life data.  

For our example, we will extract patient summaries from PubMed.  From these patient summaries, we will label those that had COVID-19 and those that did not.  Selected sentences will be used to create word embeddings.  These embeddings will be trained as part a neural network model to perform a classification task.  By training the word embeddings, the computer will learn if there are any meaningful relationships between the words in the text. 

## A short intro to Word Embeddings

Word Embeddings were a bit of a complex concept to grasp, until I got into the weeds of building one.  I've come to appreciate that they are an important concept in deep learning, for the reason that word meanings can be approximated mathematically.  

To best understand the intuition behind word embeddings, I highly recommend reading this [blog post](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations) from Christopher Olah.

There are a number of techniques available to build a word embedding, including using pre-trained embeddings generated by GloVe, word2vec.  For the purposes of this tutorial, we will train an embedding from scratch. 

As a quick note, LLMs use word embeddings, but the technique used to build them are proprietary.  

## Let's get data

I selected a dataset of ~167K PubMed patient summaries via [HuggingFace](https://huggingface.co/datasets/zhengyun21/PMC-Patients/tree/main).  To demonstrate a low-code approach, SQL Server and Azure Data Studio will be used.

The data is loaded into [SQL Server on a Mac](https://builtin.com/software-engineering-perspectives/sql-server-management-studio-mac).  Start SQL Server via the terminal, giving the username and password:

```
$ mssql -u <sql server username> -p <sql server password>
```

We then [connect](https://www.sqlshack.com/sql-server-data-import-using-azure-data-studio/) SQL Server to Azure Data Studio, where we can access and query the data.

Attached is a screenshot to modify the columns before importing the data:

![AzureDataStudio](/assets/images/2023-09/azstudio_setup3.png)

We run the below [query](https://github.com/slsu0424/pmc-patients/blob/develop/pmc-patients.sql) to extract the first 100 records.

```
SELECT TOP (100) [patient_id]
  ,[patient_uid]
  ,[PMID]
  ,[file_path]
  ,[title]
  ,[patient]
  ,[age]
  ,[gender]
  ,[similar_patients]
  ,[relevant_articles]
FROM [test].[dbo].[PMC-Patients]
```

## Define class labels

Since this is a binary classification task, we will label the dataset as:
- patients with COVID-19 = '1'
- patients without COVID-19 = '0'

```python
labels = []

df['label'] = ''

for index, row in df.iterrows():
    if 'COVID-19' in row['patient']:
        row['label'] = '1'
    else:
        row['label'] = '0' 

    labels.append(row['label'])

# convert list to array
labels_arr = np.array(labels).astype(float)

print(labels_arr)
```

```
[1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.
 0. 0. 1. 0.]
```

## Create a corpus

Now that we have our labeled dataset, the next step is to create a corpus.  We take the first 3 sentences from each record (document).  A sample of the first 3 documents in the corpus is below:

```
['This 60-year-old male was hospitalized due to moderate ARDS from COVID-19 with symptoms of fever, dry cough, and dyspnea. We encountered several difficulties during physical therapy on the acute ward.', 
'We describe the case of a 55-year-old male who presented to the emergency department via emergency medical services for the chief complaint of sudden onset shortness of breath that woke him from his sleep just prior to arrival.', 
'A 20-year-old Caucasian male (1.75 m tall and 76 kg (BMI 24.8)), was admitted to the medical department for persistent hyperpyrexia, severe sore throat, dyspnea, and impaired consciousness with stupor. Persistent symptoms started at home 4 days before and he assumed clarithromycin as empiric antibiotic therapy.', ...]
```

For 100 documents, there are 6453 total words in the corpus.


## Convert text to integers

One approach would be to one-hot encode each word, but there are limitations with this technique.  A better approach would be to tag each word with a unique integer.  The integer encoding for a specific word remains the same across all documents, so this will help reduce the size of the corpus to unique words only (vocabulary). 

To do this, Keras (neural network library) provides a handy **Tokenizer() API** that can handle multiple documents.  For a deeper understanding of how to implement this, see this [tutorial](https://machinelearningmastery.com/prepare-text-data-deep-learning-keras).

```python
encod_corp = []

# fit tokenizer on docs
t = Tokenizer()
t.fit_on_texts(corp) 
encod_corp = t.texts_to_sequences(corp) # convert docs to num sequence

# get unique words
vocab = t.word_index

print("vocab:")
for i in vocab:
    print(i)

vocab_size = len(vocab) # input into embedding layer
print('Vocab size = %s unique words' % vocab_size)

```

```
vocab:
a
and
the
of
with
was
to
year
old
in
...

Vocab size = 1843 unique words

``` 
Out of a total of 6453 words, 1843 unique words are found.

## Pad the documents

Keras requires that all documents must be the same length.  We first find the maximum length of a document, which is 145 words.  Padding (zeroes) are then added to the shorter documents using the **pad_sequences** function:

```python
# pad the docs with zeros
pad_corp=pad_sequences(encod_corp,maxlen=maxlen,padding='post',value=0.0)

print(pad_corp)
```

```
[[  58  467    8    9   24    6  309   43    7  191  192   30  119  120
     5   55    4   33  105   59    2   78  138  767  310  311   91   63
    69   19    3  121  468   92  139  469    4  241   84  166  193  768
   312  769   34  470  167  471    2   78    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0    0]
   ... 
```

This array represents the text of Document 1: 

'This 60-year-old male was hospitalized due to moderate ARDS from COVID-19 with symptoms of fever, dry cough, and dyspnea. We encountered several difficulties during physical therapy on the acute ward.'  

Since this document has only 47 words, the rest of the document is padded with zeroes.


## Create an embedding 

To create the embedding, we create a Keras Sequential model.  By sequential, this means that each layer in the network has exactly one input and one output.  To define the embedding, we need 3 inputs:

- input_dim: size of vocabulary
- output_dim: how many dimensions the word should be embedded into
- input_length: maximum length of a document

I like to think of the output_dim as a form of compression.  That is, we want to compress each word such that its meaning can be represented in N dimensions. 

To better understand the logic behind creating an embedding, I found these [articles](https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work?rq=1) particulary helpful.  

```python
# create keras model
model = Sequential()

# define embedding
embedding_layer = Embedding(
                        input_dim=vocab_size+1,
                        output_dim=2, 
                        input_length=maxlen)

# add layers
model.add(embedding_layer) # layer 0
model.add(Flatten()) # layer 1
model.add(Dense(1, activation='sigmoid'))  # layer 2

# configure the learning process
model.compile(
        optimizer='adam',
        loss='categorical_crossentropy', 
        metrics=['accuracy'])

# model prediction
embedding_output = model.predict(pad_corp)
```

## Visualize intial embeddings

The output of an embedding layer is a lookup table, which maps each word in the vocabulary to a set of random numbers in the dimension specified.  For example, since we set our output_dim = 2, we should expect to see each word mapped to 2 random numbers:

```python
# Extract embedding matrix (lookup table)

embedding_layer = model.get_layer(index=0)

embedding_matrix = embedding_layer.get_weights()[0]

print(embedding_matrix)
```
```
[[ 3.48836184e-03  4.73979823e-02]
 [ 1.17111579e-02 -4.21698801e-02]
 [ 2.09044702e-02 -4.68258746e-02]
 ...
```

Revisiting the first document, we see that each embedding value is mapped to a word in that document:




Let's see how this looks visually.  Since these embeddings are not trained, it would make sense that the words are fairly scattered:

![](https://github.com/slsu0424/pmc-patients/blob/develop/output1.png)


## Visualize trained embeddings

After adding the embedding layer, we have a N dimension.  We need to compress that into a 2D vector.  

## Conclusion

In this tutorial, we explored one-hot encoding, a very simple way to convert categorical variables for natural language processing tasks.  By taking a subset of PubMed abstracts, we were able to see how this approach becomes highly inefficient with larger vocabularies.  Some of these inefficiencies are due to a lack of understanding relationships between words, as well as a sparse and highly-dimensional feature space.

We can boost the performance of the training accuracy by adding in another layer, such as a convolution layer.  I will explore these in future posts.


## References

Data/SQL Server:
+ <https://huggingface.co/datasets/zhengyun21/PMC-Patients/tree/main>
+ <https://builtin.com/software-engineering-perspectives/sql-server-management-studio-mac>
+ <https://www.sqlshack.com/sql-server-data-import-using-azure-data-studio/>