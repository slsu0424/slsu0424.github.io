---
layout: post
title:  "Chatting with healthcare data in natural language"
author: sandy
categories: [ ChatGPT, NLP, tutorial ]
image: assets/images/2023-10/shutterstock_2188258735_license_resize.png
---
In September, I had the opportunity to present the 2nd iteration of my talk on Large Language Models (LLMs).  This time I had a chance to go a bit deeper into the demos that covered integrating LLMs with LangChain, and LLMs with SQL.  I have written this tutorial to accompany those demos.    

This tutorial will cover 2 examples:

1) Use LLMs with LangChain to chat with a healthcare document  
2) Use LLMs with SQL to chat with the MIMIC-III database

Pre-requisites:
1. Basic knowledge of OpenAI
2. Basic knowledge of Azure data services
3. Python

All resources can be found [here](https://github.com/slsu0424/pmc-patients).

## Example 1: Use LLMs with LangChain to chat with a healthcare document 

#### Generate an ADE report

I used OpenAI's ChatGPT (GPT-3.5) to generate a synthetic adverse events report for warfarin.  I chose warfarin as it is in the class of drugs that have resulted in [serious adverse drug reactions](https://www.ncbi.nlm.nih.gov/books/NBK519025/).

These were the series of prompts I used to generate the final [output]():

"Create an adverse event report related to warfarin.  Limit to 250 words."  
"Remove the Reporting Authority section"  
"expand to 500 words"  
"Include the Reporting Authority section"  

A snippet of the document is below:

>On October 20, 2023, at 09:30 AM, the patient, John Doe, experienced a significant adverse event related to the anticoagulant medication warfarin. Mr. Doe, a 68-year-old male with a history of atrial fibrillation, had been taking warfarin (5 mg daily) for the past three years as prescribed by his cardiologist.

#### Q&A application overview

This [blog post](https://github.com/hwchase17/chat-your-data/blob/master/blogpost.md) by LangChain's founder, Harrison Chase, provides a high-level overview for building a text-based Q&A application.  

The main steps include:

1. Load documents
2. Split documents into chunks
3. Create embedding vectors from chunks
4. Store vectors in vector database
5. Retrieve relevant documents from database
6. Pass relevant documents to LLM
7. LLM generates final answer

Ingest Data:

![langchain1](/assets/images/2023-10/langchain1.png)

Query Data:

![langchain2](/assets/images/2023-10/langchain2.png)


#### Use LangChain to load documents into a vector store
[Langchain](https://docs.langchain.com/docs/) is a framework for developing applications powered by LLMs, like the one above.  The main idea is that developers can "chain" different components around an LLM to create more powerful use cases.  

Hence, we can "chain" an LLM to another component, such as a document.

LangChain has many different methods to load documents.  We easily use the [PyPDFLoader](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf) to load in the PDF document and create a vector representation using the [VectorStoreIndexCreator](https://api.python.langchain.com/en/latest/indexes/langchain.indexes.vectorstore.VectorstoreIndexCreator.html):

```python
# Load PDF document
loaders = PyPDFLoader('/Users/sandysu/Documents/GitHub/OpenAI/docs/ADR11.pdf')

# Create a vector representation of the loaded document
index = VectorstoreIndexCreator().from_loaders([loaders])
```

The VectorStoreIndexCreator handles steps 2-4 above (chunking, embedding, and storage).  This [article](https://medium.com/@kbdhunga/enhancing-conversational-ai-the-power-of-langchains-question-answer-framework-4974e1cab3cf) goes into more detail about the class, including customization.  

The default settings to note are: 

- Chunking - uses [RecursiveCharacterTextSplitter()](https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.RecursiveCharacterTextSplitter.html) to divide text at specific characters
- Embedding - uses [OpenAIEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.openai.OpenAIEmbeddings.html) to generate embeddings
- Storage - embeddings are stored in [Chroma](https://www.trychroma.com/), an open-source vector store 

#### Set up Streamlit app to query document
Next, we set up a simple UI to allow users to ask questions of the ADE document.  

```python
# Display the page title and the text box for the user to ask the question
st.title('ðŸ¦œ Query your PDF document ')
prompt = st.text_input("Enter your question to query your PDF documents")
```

#### Query the vector store
The [query()](https://api.python.langchain.com/en/latest/_modules/langchain/indexes/vectorstore.html#VectorStoreIndexWrapper.query) method is used to interact with the vector store.  Thus, when a user passes in a question, the store is queried to retrieve the data that is 'most similar' to the embedded query.

```python
response = index.query(llm=OpenAI(model_name="gpt-3.5-turbo", temperature=0.2), question = prompt, chain_type = 'stuff')
```

Under the hood, we pass in the OpenAI model (gpt-3.5-turbo), and set the [temperature](https://platform.openai.com/docs/guides/gpt/how-should-i-set-the-temperature-parameter).  The temperature controls the randomness of the output generated by the model.  For example, a temperature value closer to 1 will generate a more creative resonse.  For [chain_type = 'stuff'](https://python.langchain.com/docs/modules/chains/document/stuff), this will combine multiple input documents into a single prompt to pass to an LLM.

This visual shows the workflow in more detail:

<a href="https://python.langchain.com/docs/modules/data_connection/vectorstores/#:~:text=One%20of%20the%20most%20common%20ways%20to%20store,that%20are%20%27most%20similar%27%20to%20the%20embedded%20query">
  <img src="/assets/images/2023-10/langchain3.png" alt="langchain3" width="750" height="311">
</a>


#### Let's ask some questions
Let's test this out:



## Example 2: Use LLMs with SQL to chat with the MIMIC-III database

#### Create database in Azure SQL DB
The MIMIC-III is an open database comprising of de-identified health-related data for > 40,000 patients who stayed in critical care units of the Beth Israel Deaconess Medical Center between 2001 and 2012.  I used the MIMIC-III database to  

To do this, table files can be downloaded from MIMIC.  We then load this table into Azure SQL DB.

![](/assets/images/2023-09/output2.png)

## Conclusion

In this tutorial, we explored how to create word embeddings from scratch, using a neural network to perform a classification task.  By taking sample text from PubMed patient summaries, we were able to train a neural network to classify patients who had COVID-19 and those that did not.  In doing so, we were also able to train the embeddings, such that words with similar meanings were visually placed closer together.  

We can boost the performance of the training accuracy by adding in a different layer, such as a convolution layer.  I will explore these in future posts.


## References
+ <https://medium.com/technology-hits/overview-of-langchain-9f6362707cd0>
+ <https://towardsai.net/p/machine-learning/chat-with-your-healthcare-documents-build-a-chatbot-with-chatgpt-and-langchain>

https://www.techsmartfuture.com/chunking-natural-language-processing/
https://towardsdatascience.com/how-to-chunk-text-data-a-comparative-analysis-3858c4a0997a
https://api.python.langchain.com/en/latest/indexes/langchain.indexes.vectorstore.VectorstoreIndexCreator.html

https://www.wwt.com/blog/a-brief-history-of-nlp
https://dspace.mit.edu/bitstream/handle/1721.1/150502/2023_NLP_JPM.pdf?sequence=1&isAllowed=y

https://www.analyticsvidhya.com/blog/2022/07/the-evolution-of-nlp-from-1950-to-2022/