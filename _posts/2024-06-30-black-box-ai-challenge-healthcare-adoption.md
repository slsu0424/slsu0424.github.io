---
layout: post
title:  "Black box AI: The challenge for healthcare adoption"
author: sandy
categories: [ AI, healthcare, algorithms ]
image: assets/images/2024-06/aditya-patil-QKkiJ7AXarU-unsplash_resize.jpg
---
After writing my last post, I was intrigued to learn a bit more about the black box phenomenon in AI models.  More pressing is understanding their implication for healthcare.  This is not to say that AI has not already been embedded into medical devices today, judging by the increasing number of FDA-approved AI algorithms.  However, I think it is important to understand the distinction of the algorithms embedded into such devices.  In this post, I aim to provide a technical lens into this topic.  

## A database of FDA-approved AI algorithms
For those that are not aware, the FDA now regularly tracks and publishes approved [AI/ML-enabled medical devices](https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-and-machine-learning-aiml-enabled-medical-devices).  I was also able to dig up a 2020 article in [Nature](https://www.nature.com/articles/s41746-020-00324-0) which went a step further to categorize these algorithms in a [database](https://medicalfuturist.com/fda-approved-ai-based-algorithms).

I was interested in finding devices that used deep learning, given their rise in popularity in recent years.  The database does not provide specific algorithms used, but I decided to settle on cardiovascular devices.  Eko Devices Inc. developed the Eko Analysis Software (EAS), which was approved by the FDA on January 15, 2020.  According to their [summary](https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfpmn/pmn.cfm?ID=K192004) document, the tool is described as follows:

>The Eko Analysis Software is a cloud-based software API that allows a user to upload
synchronized ECG and heart sound/phonocardiogram (PCG) data for analysis. The software
uses several methods to interpret the acquired signals including signal processing and artificial
neural networks. The API can be electronically interfaced, and perform analysis with data
transferred from multiple mobile or computer based applications. 

I will focus on the neural networks component.  

## Discriminative vs. Generative models
Before I dive into discriminative and generative models, I found it helpful to review the main approaches to perform machine learning tasks: [supervised and unsupervised learning](https://www.enjoyalgorithms.com/blogs/supervised-unsupervised-and-semisupervised-learning).  

At a high level, supervised learning uses labeled inputs (x) and outputs (y) to fit a model.  Here, the machine algorithm estimates, or learns, a function that best relates the input ("labeled data") to output ("labels"):  

![supervised](/assets/images/2024-06/supervised.png){:.centered}

In contrast, unsupervised learning has no labeled inputs nor outputs.  Here, the machine learning algorithm estimates a function that finds similarity among the input samples, and groups them based on that similarity as the output:  

![unsupervised](/assets/images/2024-06/unsupervised.png){:.centered}

{:.image-caption}
*Credit: Kumar, R. Supervised, Unsupervised and Semi-supervised Learning with Real-life Usecase.  EnjoyAlgorithms.*

The models that fall under each approach can be generally categorized as [discriminative or generative](https://learnopencv.com/generative-and-discriminative-models).  By discriminative, the model aims to separate data points into different classes and learn the decision boundaries.  By generative, the model aims to generate new data points similar to the data it was trained on:

![gen_disc_model-1](/assets/images/2024-06/gen_disc_model-1.png){:.centered}

{:.image-caption}
*Credit: LearnOpenCV Team. Generative and Discriminative Models.  LearnOpenCV (May 2021).*

[Mathematically](https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-supervised-learning), we typically see discriminative models used in supervised learning, where the goal is to estimate the conditional probability of the output (y) given the input (x), thus P(y|x).  Generative models are typically used in unsupervised learning, where the goal is to estimate the joint probability of both the input (x) and output (y), thus P(xâˆ©y).  New data points can be generated by sampling from the underlying probability distribution:

![math](/assets/images/2024-06/math.png){:.centered}

{:.image-caption}
*Credit: Amidi, S and Amidi, A. CS 229 - Machine Learning.  Stanford.*

While neural networks are typically associated with unsupervised learning, the lines are somewhat blurred as discussed in some [online forums](https://stats.stackexchange.com/questions/403968/linking-generative-discriminative-models-to-supervised-and-unsupervised-learnin).  Thus, I would say they can be either discriminative or generative models, based on the specific use case.  For example, neural networks can be used for image classification (convolutional neural networks) or for generating new data (transformers).

As for the Eko Analysis System, the underlying neural networks are probably discriminative models.  According to their [FAQ](https://support.ekohealth.com/hc/en-us/articles/13180195624347-Eko-App-FAQ), it states that "The software [EAS] detects the presence of atrial fibrillation and normal sinus rhythm from the ECG signal."  There does not seem to be any indication that the software is generating new data, but rather is classifying the data points into distinct categories.

# Black box AI vs. glass box AI
So with all this discussion on discriminative vs. generative models, why does this matter in practice?  AI systems as black boxes are nothing new, as I first encountered this during my time at IBM Watson.  Fast forward a decade, and the algorithms powering today's AI systems are far more advanced and not easily understood, even by the researchers who developed them.  

Much of this advancement is due to the "deep learning revolution" in 2012.  According to [Ars Technica](https://arstechnica.com/science/2019/12/how-neural-networks-work-and-why-theyve-become-a-big-business), simple (single-layer) neural networks were first developed in the early 1950s, but started to see more mainstream success from 2012 onwards.  The main contributors to this success were a combination of:

- Deeper networks (more layers)
- Large datasets (more data)
- Powerful compute (more processing efficiency)

Why it matters: These systems can now model much more complex relationships on their own.  On the one hand, this is much more efficient than explicit coding by a human being; on the other, it becomes harder to decipher the reasoning why the system has made the decisions it did throughout the network.  In this sense, discriminative models are more interpretable than generative models, since relationship between the inputs and outputs remains consistent (outputs are variable in generative models).

To see why this is, let's take a look at the [intricacies](https://spectrum.ieee.org/what-is-deep-learning) involved in a neural network with many layers:

![nn](/assets/images/2024-06/nn.png){:.centered}

{:.image-caption}
*Credit: Moore, S., Schneider, D., Strickland, E. How Deep Learning Works.  IEEE Spectrum (September 2021).*

Imagine trying to unravel this and explain it to a non-technical person, let alone someone in a completely different field like healthcare.

Some argue that these increasingly sophisticated AI (specifically generative) systems pose a [risk](https://www.techtarget.com/healthtechanalytics/feature/Navigating-the-black-box-AI-debate-in-healthcare) for patient care.  If an AI system makes a diagnosis, how did it come to that conclusion?  Would it come to that same conclusion the next time?  There appears to be some consensus that in low-stakes use cases, such as administrative tasks, black box AI is not much of an issue.  However, there are high-stakes for clinical decision making, where accountability and liability are still not well-defined.  As a result, the lack of transparency may hinder more widespread adoption of healthcare AI.

Healthcare would be better served with [glass box AI](https://theconversation.com/what-is-a-black-box-a-computer-scientist-explains-what-it-means-when-the-inner-workings-of-ais-are-hidden-203888), where the algorithms, training data, and the model are freely available.  However, this poses challenges for companies who are looking to protect their IP.  There is on-going development around explainable AI to help mitigate these risks, although it remains to be seen what will constitute a satisfactory explanation.

## Conclusion
Exploring the black box phenomenon really opened my eyes into the technical advances and societal challenges posed by AI.  As with many emerging technologies that have come before it, these are now indispensible in our everyday lives.  Likewise, society has had to adjust to the ill-effects of such technology.  Whether that's through regulation, education, or testing frameworks, I remain bullish that these advances in AI will be a boon for healthcare.

## References
+ <https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-and-machine-learning-aiml-enabled-medical-devices>
+ <https://www.nature.com/articles/s41746-020-00324-0>
+ <https://medicalfuturist.com/fda-approved-ai-based-algorithms>
+ <https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfpmn/pmn.cfm?ID=K192004>
+ <https://learnopencv.com/generative-and-discriminative-models>
+ <https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-supervised-learning>
+ <https://stats.stackexchange.com/questions/403968/linking-generative-discriminative-models-to-supervised-and-unsupervised-learnin>
+ <https://arstechnica.com/science/2019/12/how-neural-networks-work-and-why-theyve-become-a-big-business>
+ <https://spectrum.ieee.org/what-is-deep-learning>
+ <https://www.techtarget.com/healthtechanalytics/feature/Navigating-the-black-box-AI-debate-in-healthcare>
+ <https://theconversation.com/what-is-a-black-box-a-computer-scientist-explains-what-it-means-when-the-inner-workings-of-ais-are-hidden-203888>
